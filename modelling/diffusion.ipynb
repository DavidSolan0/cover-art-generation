{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dsola\\OneDrive\\Documentos\\david_projects\\art-cover-generation\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from diffusion_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "n_feat = 64  # 64 hidden dimension feature\n",
    "n_cfeat = 1  # context vector is of size 5\n",
    "height = 64  # 16x16 image\n",
    "save_dir = \"./weights/\"\n",
    "dataroot = \"../dataset\"\n",
    "workers = 4\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 200\n",
    "lrate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(\n",
    "    root=dataroot,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(height),\n",
    "            transforms.CenterCrop(height),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, n_feat=256, n_cfeat=10, height=28\n",
    "    ):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)  # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)  # down2 #[10, 256, 4,  4]\n",
    "\n",
    "        # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(\n",
    "            nn.AvgPool2d((16)), nn.GELU()\n",
    "        )  # height of thw down2 layer\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2 * n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1 * n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2 * n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1 * n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * n_feat, 2 * n_feat, self.h // 4, self.h // 4\n",
    "            ),  # up-sample\n",
    "            nn.GroupNorm(8, 2 * n_feat),  # normalize\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                2 * n_feat, n_feat, 3, 1, 1\n",
    "            ),  # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat),  # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                n_feat, self.in_channels, 3, 1, 1\n",
    "            ),  # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)  # [10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)  # [10, 256, 4, 4]\n",
    "\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(\n",
    "            -1, self.n_feat * 2, 1, 1\n",
    "        )  # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        # print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        j = cemb1 * up1 + temb1\n",
    "        up2 = self.up1(cemb1 * up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2 * up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# re setup optimizer\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return (\n",
    "        ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [01:09<40:37, 69.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/36 [01:59<32:42, 57.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/36 [02:47<29:23, 53.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/36 [03:39<28:11, 52.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [04:28<26:35, 51.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [05:12<24:28, 48.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/36 [05:52<22:15, 46.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [06:34<20:55, 44.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [07:17<19:52, 44.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [08:00<19:00, 43.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [08:46<18:30, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 12/36 [09:34<18:12, 45.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [10:19<17:27, 45.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [11:03<16:29, 44.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [11:46<15:33, 44.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 16/36 [12:30<14:45, 44.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [13:17<14:14, 44.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [14:00<13:20, 44.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [14:44<12:34, 44.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [15:28<11:44, 44.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [16:12<11:03, 44.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [16:55<10:14, 43.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23/36 [17:39<09:29, 43.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [18:27<08:59, 44.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [19:11<08:12, 44.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [20:00<07:41, 46.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 27/36 [20:47<06:55, 46.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28/36 [21:31<06:05, 45.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 29/36 [22:17<05:19, 45.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 30/36 [23:02<04:33, 45.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 31/36 [23:48<03:47, 45.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 32/36 [24:33<03:01, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33/36 [25:17<02:15, 45.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 34/36 [26:03<01:30, 45.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 35/36 [26:48<00:45, 45.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([82, 128, 16, 16])\n",
      "torch.Size([82, 128, 1, 1])\n",
      "Djijii shape: torch.Size([82, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [27:25<00:00, 45.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at ./weights/model_0.pth\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [01:07<39:24, 67.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/36 [01:53<31:04, 54.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/36 [02:42<28:48, 52.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/36 [03:34<27:40, 51.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [04:26<26:52, 52.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [05:18<26:05, 52.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/36 [06:16<26:07, 54.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [07:08<24:52, 53.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [07:58<23:32, 52.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [08:53<22:58, 53.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [09:43<21:43, 52.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 12/36 [10:33<20:34, 51.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [11:24<19:44, 51.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [12:14<18:42, 51.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [13:04<17:40, 50.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 16/36 [13:52<16:38, 49.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [14:42<15:48, 49.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [15:31<14:54, 49.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [16:20<14:00, 49.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [17:09<13:08, 49.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [17:58<12:16, 49.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [18:45<11:21, 48.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23/36 [19:33<10:26, 48.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [20:21<09:41, 48.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [21:16<09:13, 50.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [22:03<08:12, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 27/36 [22:52<07:23, 49.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28/36 [23:42<06:34, 49.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 29/36 [24:28<05:38, 48.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 30/36 [25:20<04:57, 49.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 31/36 [26:08<04:04, 48.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 32/36 [26:52<03:10, 47.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33/36 [27:36<02:19, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 34/36 [28:20<01:31, 45.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 35/36 [29:04<00:45, 45.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([82, 128, 16, 16])\n",
      "torch.Size([82, 128, 1, 1])\n",
      "Djijii shape: torch.Size([82, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [29:36<00:00, 49.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [01:06<38:38, 66.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/36 [01:48<29:40, 52.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/36 [02:30<26:09, 47.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/36 [03:14<24:33, 46.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [03:59<23:33, 45.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [04:42<22:22, 44.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/36 [05:23<21:05, 43.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [06:12<21:03, 45.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [06:57<20:24, 45.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [07:41<19:27, 44.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [08:26<18:41, 44.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 12/36 [09:09<17:42, 44.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [09:51<16:44, 43.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [10:35<16:01, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [11:17<15:07, 43.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 16/36 [11:59<14:15, 42.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [12:41<13:27, 42.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [13:24<12:51, 42.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [14:08<12:10, 42.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [14:51<11:28, 43.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [15:35<10:50, 43.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [16:23<10:28, 44.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23/36 [17:10<09:51, 45.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [17:54<08:58, 44.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [18:44<08:30, 46.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [19:34<07:56, 47.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down2 shape: torch.Size([100, 128, 16, 16])\n",
      "torch.Size([100, 128, 1, 1])\n",
      "Djijii shape: torch.Size([100, 128, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [20:25<07:51, 47.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39m# loss is mean squared error between the predicted and true noise\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(pred_noise, noise)\n\u001b[1;32m---> 26\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     28\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m \u001b[39m# save model periodically\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dsola\\OneDrive\\Documentos\\david_projects\\art-cover-generation\\venv\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Dsola\\OneDrive\\Documentos\\david_projects\\art-cover-generation\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training without context code\n",
    "# set into train mode\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f\"epoch {ep}\")\n",
    "\n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0][\"lr\"] = lrate * (1 - ep / n_epoch)\n",
    "\n",
    "    pbar = tqdm(dataloader, mininterval=2)\n",
    "    for x, _ in pbar:  # x: images\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "\n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "\n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps)\n",
    "\n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    # save model periodically\n",
    "    if ep % 4 == 0 or ep == int(n_epoch - 1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "        print(\"saved model at \" + save_dir + f\"model_{ep}.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f\"sampling timestep {i:3d}\", end=\"\\r\")\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)  # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate == 0 or i == timesteps or i < 8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "# nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(\n",
    "    intermediate_ddpm, 32, 4, save_dir, \"ani_run\", None, save=False\n",
    ")\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(\n",
    "    torch.load(f\"{save_dir}/model_trained.pth\", map_location=device)\n",
    ")\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f\"sampling timestep {i:3d}\", end=\"\\r\")\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)  # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate == 0 or i == timesteps or i < 8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(\n",
    "    intermediate_ddpm, 32, 4, save_dir, \"ani_run\", None, save=False\n",
    ")\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset neural network\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# re setup optimizer\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with context code\n",
    "# set into train mode\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f\"epoch {ep}\")\n",
    "\n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0][\"lr\"] = lrate * (1 - ep / n_epoch)\n",
    "\n",
    "    pbar = tqdm(dataloader, mininterval=2)\n",
    "    for x, c in pbar:  # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        c = c.to(x)\n",
    "\n",
    "        # randomly mask out c\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
    "        c = c * context_mask.unsqueeze(-1)\n",
    "\n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "\n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps, c=c)\n",
    "\n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    # save model periodically\n",
    "    if ep % 4 == 0 or ep == int(n_epoch - 1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"context_model_{ep}.pth\")\n",
    "        print(\"saved model at \" + save_dir + f\"context_model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in pretrain model weights and set to eval mode\n",
    "nn_model.load_state_dict(\n",
    "    torch.load(f\"{save_dir}/context_model_trained.pth\", map_location=device)\n",
    ")\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Context Model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with context using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_context(n_sample, context, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f\"sampling timestep {i:3d}\", end=\"\\r\")\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)  # predict noise e_(x_t,t, ctx)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate == 0 or i == timesteps or i < 8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples with randomly selected context\n",
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddpm_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(\n",
    "    intermediate, 32, 4, save_dir, \"ani_run\", None, save=False\n",
    ")\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, nrow=2):\n",
    "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4, 2))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(imgs, axs):\n",
    "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined context\n",
    "ctx = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            # hero, non-hero, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "        ]\n",
    "    )\n",
    "    .float()\n",
    "    .to(device)\n",
    ")\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix of defined context\n",
    "ctx = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            # hero, non-hero, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],  # human\n",
    "            [1, 0, 0.6, 0, 0],\n",
    "            [0, 0, 0.6, 0.4, 0],\n",
    "            [1, 0, 0, 0, 1],\n",
    "            [1, 1, 0, 0, 0],\n",
    "            [1, 0, 0, 1, 0],\n",
    "        ]\n",
    "    )\n",
    "    .float()\n",
    "    .to(device)\n",
    ")\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampling function for DDIM\n",
    "# removes the noise using ddim\n",
    "def denoise_ddim(x, t, t_prev, pred_noise):\n",
    "    ab = ab_t[t]\n",
    "    ab_prev = ab_t[t_prev]\n",
    "\n",
    "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
    "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
    "\n",
    "    return x0_pred + dir_xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model without context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample quickly using DDIM\n",
    "@torch.no_grad()\n",
    "def sample_ddim(n_sample, n=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f\"sampling timestep {i:3d}\", end=\"\\r\")\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t)  # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddim(32, n=25)\n",
    "animation_ddim = plot_sample(intermediate, 32, 4, save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(\n",
    "    torch.load(f\"{save_dir}/context_model_31.pth\", map_location=device)\n",
    ")\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Context Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast sampling algorithm with context\n",
    "@torch.no_grad()\n",
    "def sample_ddim_context(n_sample, context, n=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f\"sampling timestep {i:3d}\", end=\"\\r\")\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)  # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddim_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(\n",
    "    intermediate, 32, 4, save_dir, \"ani_run\", None, save=False\n",
    ")\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
